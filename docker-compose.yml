version: '3.7'
services:
  postgres:
    image: postgres:13.2-alpine
    container_name: airglue_postgres
    restart: always
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    logging:
      options:
        max-size: 10m
        max-file: "3"

  webserver:
    build: infrastructure/docker/release/${AIRGLUE_COMPOSER_AIRFLOW_VERSION}
    container_name: airglue_webserver
    restart: always
    entrypoint: ./script/entrypoint.sh
    depends_on:
        - postgres
    environment:
      # Airflow
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOGGING_LEVEL=INFO
      # Airglue specific
      - AIRGLUE_SRC_PATH=/opt/airflow/dags
      - AIRGLUE_CONFIG_PATH=/opt/airflow/dags/airglue/example # Example config path
      # Example DAG
      - AIRFLOW_CONN_BIGQUERY_DEFAULT=google-cloud-platform://?extra__google_cloud_platform__project=${AIRGLUE_SANDBOX_PROJECT_ID?AIRGLUE_SANDBOX_PROJECT_ID_is_undefined}
      - AIRFLOW_CONN_AIRFLOW_POSTGRES=postgres://airflow:airflow@postgres:5432/airflow
      - AIRFLOW_VAR_EXAMPLE_BUCKET_NAME=${AIRGLUE_EXAMPLE_BUCKET_NAME}
      - AIRGLUE_SANDBOX_PROJECT_ID=${AIRGLUE_SANDBOX_PROJECT_ID}
      - GOOGLE_APPLICATION_CREDENTIALS=/secret/gcp-credentials/airglue-sandbox-sa.json
    logging:
      options:
        max-size: 10m
        max-file: "3"
    volumes:
      - ./airglue:/opt/airflow/dags/airglue
      - ~/.config/gcloud/airglue/airglue-sandbox-sa.json:/secret/gcp-credentials/airglue-sandbox-sa.json # mount sandbox service account
    ports:
      - "8082:8080"
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3