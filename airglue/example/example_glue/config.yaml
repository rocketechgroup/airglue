enabled: true
schedule_interval: "0 2 * * *"
timezone: "Europe/London"
description: "## This is my example DAG"
params:
  default_dataset: airglue_example
envs:
  - AIRGLUE_SANDBOX_PROJECT_ID
vars:
  - example_bucket_name
tasks:
  - identifier: waiting_for_external_dag_to_complete
    operator: airflow.sensors.external_task_sensor.ExternalTaskSensor
    arguments:
      external_dag_id: airglue__example_upstream_dependency
      check_existence: true

  - identifier: gcs_to_bq_example_us_states
    operator: airflow.contrib.operators.gcs_to_bq.GoogleCloudStorageToBigQueryOperator
    arguments:
      bucket: "cloud-samples-data"
      source_objects:
        - "bigquery/us-states/us-states.csv"
      destination_project_dataset_table: "{{ params.default_dataset }}.gcs_to_bq_table_us_states_csv"
      write_disposition: "WRITE_TRUNCATE"
      source_format: "CSV"
      skip_leading_rows: 1
      schema_fields:
        - name: "name"
          type: "STRING"
          mode: "NULLABLE"
        - name: "post_abbr"
          type: "STRING"
          mode: "NULLABLE"
    dependencies:
      - waiting_for_external_dag_to_complete

  - identifier: gcs_to_bq_example_us_states_avro
    operator: airflow.contrib.operators.gcs_to_bq.GoogleCloudStorageToBigQueryOperator
    arguments:
      bucket: "cloud-samples-data"
      source_objects:
        - "bigquery/us-states/us-states.avro"
      destination_project_dataset_table: "{{ params.default_dataset }}.gcs_to_bq_table_us_states_avro"
      write_disposition: "WRITE_TRUNCATE"
      source_format: "AVRO"
    dependencies:
      - waiting_for_external_dag_to_complete

  - identifier: postgres_to_gcs_airflow_db_dag
    operator: airflow.contrib.operators.postgres_to_gcs_operator.PostgresToGoogleCloudStorageOperator
    arguments:
      postgres_conn_id: airflow_postgres
      sql: SELECT * FROM dag;
      filename: postgres_to_gcs_airflow_db_dag/{{ ds }}/dag_{}
      bucket: "{{ params.vars.example_bucket_name }}"
    dependencies:
      - waiting_for_external_dag_to_complete

  - identifier: gcs_to_bq_airflow_db_dag
    operator: airflow.contrib.operators.gcs_to_bq.GoogleCloudStorageToBigQueryOperator
    arguments:
      bucket: "{{ params.vars.example_bucket_name }}"
      source_objects:
        - "postgres_to_gcs_airflow_db_dag/{{ ds }}/dag_*"
      destination_project_dataset_table: "{{ params.default_dataset }}.postgres_to_gcs_airflow_db_dag"
      source_format: "NEWLINE_DELIMITED_JSON"
      write_disposition: "WRITE_TRUNCATE"
    dependencies:
      - postgres_to_gcs_airflow_db_dag

  - identifier: postgres_to_gcs_airflow_db_log
    operator: airflow.contrib.operators.postgres_to_gcs_operator.PostgresToGoogleCloudStorageOperator
    arguments:
      postgres_conn_id: airflow_postgres
      sql: SELECT * FROM log WHERE DATE(execution_date) = '{{ ds }}';
      filename: postgres_to_gcs_airflow_db_log/{{ ds }}/log_{}
      bucket: "{{ params.vars.example_bucket_name }}"

  - identifier: gcs_to_bq_airflow_db_log
    operator: airflow.contrib.operators.gcs_to_bq.GoogleCloudStorageToBigQueryOperator
    arguments:
      bucket: "{{ params.vars.example_bucket_name }}"
      source_objects:
        - "postgres_to_gcs_airflow_db_log/{{ ds }}/log_*"
      destination_project_dataset_table: "{{ params.default_dataset }}.postgres_to_gcs_airflow_db_log${{ ds_nodash }}"
      source_format: "NEWLINE_DELIMITED_JSON"
      write_disposition: "WRITE_TRUNCATE"
      time_partitioning:
        type: DAY
    dependencies:
      - postgres_to_gcs_airflow_db_log

  - identifier: bq_query_to_table_airflow_dag_info
    operator: airglue.contrib.operator.bigquery.query_runner.QueryToTable
    operator_factory: airglue.contrib.operator_factory.sql_file.SqlFileOperatorFactory
    arguments:
      sql_file_path: sql/dag_info_from_log.sql
      destination_dataset_table: "{{ params.default_dataset }}.bq_query_to_table_airflow_dag_info${{ ds_nodash }}"
      write_disposition: "WRITE_TRUNCATE"
      time_partitioning:
        type: DAY
        field: source_timestamp
    dependencies:
      - postgres_to_gcs_airflow_db_log

  - identifier: run_example_python_function
    description: "This task shows how to call an pre defined python function using a Operator Factory"
    operator: airflow.operators.python_operator.PythonOperator
    operator_factory: airglue.contrib.operator_factory.python.PythonOperatorFactory
    arguments:
      python_callable: example_python_function

  - identifier: done
    operator: airflow.operators.dummy_operator.DummyOperator
    dependencies:
      - gcs_to_bq_example_us_states
      - gcs_to_bq_example_us_states_avro
      - gcs_to_bq_airflow_db_dag
      - bq_query_to_table_airflow_dag_info
      - run_example_python_function

